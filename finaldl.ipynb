{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAPPING IMAGE AND CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\sujan.s\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\sujan.s\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\sujan.s\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\sujan.s\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sujan.s\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sujan.s\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          N   P   K  temperature   humidity        ph    rainfall  \\\n",
      "0        90  42  43    20.879744  82.002744  6.502985  202.935536   \n",
      "100      85  58  41    21.770462  80.319644  7.038096  226.655537   \n",
      "200      60  55  44    23.004459  82.320763  7.840207  263.964248   \n",
      "300      74  35  40    26.491096  80.158363  6.980401  242.864034   \n",
      "400      78  42  42    20.130175  81.604873  7.628473  262.717340   \n",
      "...     ...  ..  ..          ...        ...       ...         ...   \n",
      "219500  107  34  32    26.774637  66.413269  6.780064  177.774507   \n",
      "219600   99  15  27    27.417112  56.636362  6.086922  127.924610   \n",
      "219700  118  33  30    24.131797  67.225123  6.362608  173.322839   \n",
      "219800  117  32  34    26.272418  52.127394  6.758793  127.175293   \n",
      "219900  104  18  30    23.603016  60.396475  6.779833  140.937041   \n",
      "\n",
      "                     Label  \n",
      "0       Apple___Apple_scab  \n",
      "100     Apple___Apple_scab  \n",
      "200     Apple___Apple_scab  \n",
      "300     Apple___Apple_scab  \n",
      "400     Apple___Apple_scab  \n",
      "...                    ...  \n",
      "219500    Potato___healthy  \n",
      "219600    Potato___healthy  \n",
      "219700    Potato___healthy  \n",
      "219800    Potato___healthy  \n",
      "219900    Potato___healthy  \n",
      "\n",
      "[2200 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Package\\plant_disease_multimodal_dataset.csv\")\n",
    "\n",
    "columns_to_remove = [\"Mapped Label\", \"Image Path\"]  \n",
    "df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "# Remove exact duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(df)  # Check the new dataset size\n",
    "df.to_csv(\"updated_file.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print((df[\"Label\"] == \"Potato___healthy\").sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          N   P   K  temperature   humidity        ph    rainfall  \\\n",
      "0        90  42  43    20.879744  82.002744  6.502985  202.935536   \n",
      "100      85  58  41    21.770462  80.319644  7.038096  226.655537   \n",
      "200      60  55  44    23.004459  82.320763  7.840207  263.964248   \n",
      "300      74  35  40    26.491096  80.158363  6.980401  242.864034   \n",
      "400      78  42  42    20.130175  81.604873  7.628473  262.717340   \n",
      "...     ...  ..  ..          ...        ...       ...         ...   \n",
      "219500  107  34  32    26.774637  66.413269  6.780064  177.774507   \n",
      "219600   99  15  27    27.417112  56.636362  6.086922  127.924610   \n",
      "219700  118  33  30    24.131797  67.225123  6.362608  173.322839   \n",
      "219800  117  32  34    26.272418  52.127394  6.758793  127.175293   \n",
      "219900  104  18  30    23.603016  60.396475  6.779833  140.937041   \n",
      "\n",
      "                     Label  \n",
      "0       Apple___Apple_scab  \n",
      "100     Apple___Apple_scab  \n",
      "200     Apple___Apple_scab  \n",
      "300     Apple___Apple_scab  \n",
      "400     Apple___Apple_scab  \n",
      "...                    ...  \n",
      "219500    Potato___healthy  \n",
      "219600    Potato___healthy  \n",
      "219700    Potato___healthy  \n",
      "219800    Potato___healthy  \n",
      "219900    Potato___healthy  \n",
      "\n",
      "[2200 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New CSV saved with 2200 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define dataset path\n",
    "dataset_folder = r\"C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Package\\color\"\n",
    "\n",
    "# Get all image paths\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for root, _, files in os.walk(dataset_folder):\n",
    "    for file in files:\n",
    "        if file.endswith((\".jpg\", \".png\", \".jpeg\",\".JPG\",\".PNG\",\".JPEG\")):  # Adjust extensions as needed\n",
    "            full_path = os.path.abspath(os.path.join(root, file))  # Ensure full absolute path\n",
    "            image_paths.append(full_path)\n",
    "\n",
    "            # Extract folder name as label\n",
    "            label = os.path.basename(root)  \n",
    "            labels.append(label)\n",
    "\n",
    "# Create new CSV\n",
    "df = pd.DataFrame({\"image_path\": image_paths, \"Label\": labels})\n",
    "\n",
    "# Save CSV\n",
    "df.to_csv(\"new_mapped_data.csv\", index=False)\n",
    "print(f\"✅ New CSV saved with {len(df)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = r\"C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Package\\new_mapped_data.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "df[\"image_path\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged! Saved as mapped_data_with_images.csv\n",
      "    N   P   K  temperature   humidity        ph    rainfall  \\\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536   \n",
      "1  79  42  37    24.873007  82.840226  6.587919  295.609449   \n",
      "2  67  45  38    22.727910  82.170688  7.300411  260.887506   \n",
      "3  61  52  41    24.976695  83.891805  6.880431  204.800185   \n",
      "4  91  56  37    23.431916  80.568878  6.363472  269.503916   \n",
      "\n",
      "                Label                                         image_path  \n",
      "0  Apple___Apple_scab  C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Pack...  \n",
      "1  Apple___Apple_scab  C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Pack...  \n",
      "2  Apple___Apple_scab  C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Pack...  \n",
      "3  Apple___Apple_scab  C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Pack...  \n",
      "4  Apple___Apple_scab  C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Pack...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "updated_csv_file = r\"C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Package\\updated_file.csv\"\n",
    "new_mapped_csv = \"new_mapped_data.csv\" \n",
    "final_csv_output = \"mapped_data_with_images.csv\"\n",
    "\n",
    "\n",
    "df_numerical = pd.read_csv(updated_csv_file)  \n",
    "df_images = pd.read_csv(new_mapped_csv)  \n",
    "\n",
    "\n",
    "if len(df_numerical) != 2200:\n",
    "    print(f\"Error: updated_file.csv has {len(df_numerical)} rows, expected 2200.\")\n",
    "if len(df_images) != 2200:\n",
    "    print(f\"Error: new_mapped_data.csv has {len(df_images)} rows, expected 2200.\")\n",
    "\n",
    "\n",
    "df_numerical = df_numerical.sort_values(by=\"Label\").reset_index(drop=True)\n",
    "df_images = df_images.sort_values(by=\"Label\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_numerical[\"image_path\"] = df_images[\"image_path\"]\n",
    "df_numerical.to_csv(final_csv_output, index=False)\n",
    "\n",
    "print(f\"Successfully merged! Saved as {final_csv_output}\")\n",
    "print(df_numerical.head())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = r\"C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Package\\mapped_data_with_images.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "df[\"image_path\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGE PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sujan.S\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([32, 3, 224, 224])\n",
      "Sample Labels: ('Apple___healthy', 'Peach___Bacterial_spot', 'Grape___healthy', 'Grape___Black_rot', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Define Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images\n",
    "    transforms.ToTensor(),          # Convert to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Custom Dataset Class for Image + CSV Data\n",
    "class CropDiseaseDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)  \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"image_path\"]\n",
    "        label = self.df.iloc[idx][\"Label\"]  # Crop Disease Label\n",
    "\n",
    "        # Load Image using OpenCV (Faster than PIL)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        image = Image.fromarray(image)  # Convert to PIL Image\n",
    "\n",
    "        image = self.transform(image)  # Apply transformations\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load Dataset\n",
    "csv_file = \"mapped_data_with_images.csv\"\n",
    "dataset = CropDiseaseDataset(csv_file)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Check if preprocessing works\n",
    "sample_image, sample_label = next(iter(dataloader))\n",
    "print(f\"Image Shape: {sample_image.shape}\")  \n",
    "print(f\"Sample Labels: {sample_label[:5]}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Sujan.S\\AppData\\Local\\Programs\\Python\\Python312\\Scripts\\pip.exe\\__main__.py\", line 4, in <module>\n",
      "ModuleNotFoundError: No module named 'pip'\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall -y sympy torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Sujan.S\\AppData\\Local\\Programs\\Python\\Python312\\Scripts\\pip.exe\\__main__.py\", line 4, in <module>\n",
      "ModuleNotFoundError: No module named 'pip'\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision sympy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train set size: 1760 images\n",
      "✅ Test set size: 440 images\n",
      "Image Shape: torch.Size([8, 3, 224, 224])\n",
      "Numerical Features Shape: torch.Size([8, 7])\n",
      "Labels: tensor([ 0, 11,  3,  6, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"mapped_data_with_images.csv\")\n",
    "\n",
    "# Encode labels (convert category names to numbers)\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Label\"] = label_encoder.fit_transform(df[\"Label\"])  # Converts categorical labels to numerical\n",
    "\n",
    "# Split into train (80%) and test (20%)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"Label\"], random_state=42)\n",
    "\n",
    "# Define data transformations (Resizing and Normalization for images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "# Define the Dataset class\n",
    "class CropDiseaseDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx][\"image_path\"]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Select numerical features (excluding 'image_path' & 'Label')\n",
    "        numerical_features = self.df.iloc[idx, 0:-2]  \n",
    "        \n",
    "        #print(\"\\n\\t\",numerical_features)\n",
    "\n",
    "        # Convert all non-numeric values to NaN and then replace NaNs with mean values\n",
    "        numerical_features = pd.to_numeric(numerical_features, errors=\"coerce\").fillna(0)\n",
    "\n",
    "        # Convert to tensor\n",
    "        numerical_features = torch.tensor(numerical_features.values, dtype=torch.float32)\n",
    "\n",
    "        # Get encoded label\n",
    "        label = self.df.iloc[idx][\"Label\"]\n",
    "\n",
    "        return image, numerical_features, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create Train and Test Datasets\n",
    "train_dataset = CropDiseaseDataset(train_df, transform=transform)\n",
    "test_dataset = CropDiseaseDataset(test_df, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"✅ Train set size: {len(train_dataset)} images\")\n",
    "print(f\"✅ Test set size: {len(test_dataset)} images\")\n",
    "\n",
    "# Check the structure\n",
    "sample_img, sample_features, sample_label = next(iter(train_loader))\n",
    "print(f\"Image Shape: {sample_img.shape}\")  \n",
    "print(f\"Numerical Features Shape: {sample_features.shape}\")  \n",
    "print(f\"Labels: {sample_label[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CropDiseaseModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(CropDiseaseModel, self).__init__()\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.image_feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),  # Reduce spatial size\n",
    "            nn.Flatten()  \n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(num_features, 64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128 * 7 * 7 + 64, num_classes)  # Match flattened image feature size\n",
    "          \n",
    "    def forward(self, image, features):\n",
    "        img_features = self.image_feature_extractor(image)\n",
    "        img_features = img_features.view(img_features.shape[0], -1)  # Ensure it's properly flattened\n",
    "        num_features = F.relu(self.fc1(features))\n",
    "        combined_features = torch.cat((img_features, num_features), dim=1)\n",
    "        output = self.fc2(combined_features)\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model Device: cpu\n",
      "Images Device: cpu\n",
      "Features Device: cpu\n",
      "Labels Device: cpu\n",
      "Epoch 1, Loss: 195.2073\n",
      "Epoch 2, Loss: 42.6917\n",
      "Epoch 3, Loss: 27.0598\n",
      "Epoch 4, Loss: 21.4815\n",
      "Epoch 5, Loss: 15.4536\n",
      "Epoch 6, Loss: 11.9104\n",
      "Epoch 7, Loss: 14.7671\n",
      "Epoch 8, Loss: 17.6972\n",
      "Epoch 9, Loss: 9.6307\n",
      "Epoch 10, Loss: 9.2380\n",
      "Training completed on CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Ensure the code runs on CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Initialize Model\n",
    "model = CropDiseaseModel(num_features=7, num_classes=22)\n",
    "model.to(device)\n",
    "\n",
    "# Verify Model Device\n",
    "assert next(model.parameters()).device == device, \"Model is not on CPU!\"\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model Device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Check Data Device\n",
    "for images, features, labels in train_loader:\n",
    "    print(f\"Images Device: {images.device}\")\n",
    "    print(f\"Features Device: {features.device}\")\n",
    "    print(f\"Labels Device: {labels.device}\")\n",
    "    break  # Print once for confirmation\n",
    "\n",
    "# No need for CUDA memory management on CPU\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, features, labels in train_loader:\n",
    "        # Ensure everything is on CPU\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, features)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'crop_disease_model.pth'\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'crop_disease_model.pth')\n",
    "print(\"Model saved as 'crop_disease_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9795454545454545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "dataset = CropDiseaseDataset(\"mapped_data_with_images.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "preds, targets = [], []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, features, labels in test_loader:\n",
    "        images, features = images.to(device), features.to(device)\n",
    "        outputs = model(images, features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(targets, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Disease: Common Rust\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_model(model_path, num_features=7, num_classes=22):\n",
    "    model = CropDiseaseModel(num_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_crop_disease(image_path, model_path):\n",
    "    # Load model\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image, torch.zeros((1, 7)))  # Assuming 7 feature inputs\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Class labels\n",
    "    disease_names = ['Healthy', 'Bacterial Blight', 'Leaf Rust', 'Powdery Mildew', \n",
    "                      'Downy Mildew', 'Gray Leaf Spot', 'Common Rust', 'Northern Leaf Blight', \n",
    "                      'Corn Smut', 'Leaf Scald', 'Blast', 'Brown Spot', 'Tungro', 'Yellow Leaf Streak',\n",
    "                      'Smut', 'Red Rot', 'Anthracnose', 'Wilt', 'Curly Top', 'Leaf Spot', 'Mosaic', 'Other']\n",
    "    \n",
    "    return disease_names[predicted.item()]\n",
    "\n",
    "# Example usage:\n",
    "result = predict_crop_disease(\n",
    "    r'C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Package\\color\\Apple___healthy\\ec41f18f-cd53-43db-ac79-954871449e40___RS_HL 7789.JPG',\n",
    "    'crop_disease_model.pth'\n",
    ")\n",
    "print(f\"Predicted Disease: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class CropDiseaseModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(CropDiseaseModel, self).__init__()\n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor.fc = nn.Linear(512, 128)\n",
    "        self.fc1 = nn.Linear(num_features, 64)\n",
    "        self.fc2 = nn.Linear(128 + 64, num_classes)\n",
    "\n",
    "    def forward(self, image, features):\n",
    "        img_features = self.feature_extractor(image)\n",
    "        features = F.relu(self.fc1(features))\n",
    "        combined = torch.cat((img_features, features), dim=1)\n",
    "        return self.fc2(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Features: tensor([[  6.0000, 123.0000, 203.0000,  12.7568,  81.6250,   6.1303,  66.7784],\n",
      "        [ 15.0000, 140.0000, 195.0000,  13.2850,  83.5419,   5.6995,  65.8001],\n",
      "        [ 88.0000,  17.0000,  52.0000,  29.9042,  90.7528,   6.6470,  25.3783],\n",
      "        [ 26.0000,  35.0000,  31.0000,  33.4462,  53.0598,   5.3396,  98.0509],\n",
      "        [ 95.0000,  39.0000,  36.0000,  23.8633,  83.1525,   5.5614, 285.2494],\n",
      "        [ 26.0000,  68.0000,  24.0000,  28.0485,  64.0769,   7.5049,  37.1582],\n",
      "        [ 30.0000,  65.0000,  25.0000,  32.8873,  64.5946,   7.7065,  71.5057],\n",
      "        [  0.0000,  26.0000,  31.0000,  25.0707,  95.0216,   5.5479, 192.9036]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Features:\", features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1619495624.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    outputs = model(, torch.zeros((1, 7)))\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(, torch.zeros((1, 7)))\n",
    "    print(\"Outputs:\", outputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print(\"Predicted Class Index:\", predicted.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=22):\n",
    "        super(CustomCNNModel, self).__init__()\n",
    "        self.image_feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 256)  # Adjust based on input image size\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image_feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomCNNModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomCNNModel\u001b[49m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22\u001b[39m)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop_disease_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CustomCNNModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = CustomCNNModel(num_classes=22)\n",
    "model.load_state_dict(torch.load(\"crop_disease_model.pth\", map_location='cpu'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CropDiseaseModel(nn.Module):\n",
    "    def __init__(self, num_classes=22):\n",
    "        super(CropDiseaseModel, self).__init__()\n",
    "        self.image_feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 256)  # Adjust based on image size\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image_feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_feature_extractor.0.weight: torch.Size([64, 3, 3, 3])\n",
      "image_feature_extractor.0.bias: torch.Size([64])\n",
      "image_feature_extractor.3.weight: torch.Size([128, 64, 3, 3])\n",
      "image_feature_extractor.3.bias: torch.Size([128])\n",
      "fc1.weight: torch.Size([64, 7])\n",
      "fc1.bias: torch.Size([64])\n",
      "fc2.weight: torch.Size([22, 6336])\n",
      "fc2.bias: torch.Size([22])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(r\"C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Package\\crop_disease_model.pth\", map_location='cpu')\n",
    "\n",
    "for key, value in state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDiseaseModelWeird(nn.Module):\n",
    "    def __init__(self, num_classes=22):\n",
    "        super(CropDiseaseModelWeird, self).__init__()\n",
    "        self.image_feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # [3 → 64]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # [64 → 128]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(7, 64)   # matches saved weights\n",
    "        self.fc2 = nn.Linear(64, 22)  # matches saved weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image_feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)   # Flatten\n",
    "        x = F.relu(self.fc1(x[:, :7]))  # ⚠️ HACK: Only take 7 values to match fc1\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CropDiseaseModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([64, 7]) from checkpoint, the shape in current model is torch.Size([64, 6336]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([22, 6336]) from checkpoint, the shape in current model is torch.Size([22, 64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CropDiseaseModel(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSujan.S\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDL Package1\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDL Package\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcrop_disease_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Sujan.S\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2577\u001b[0m             ),\n\u001b[0;32m   2578\u001b[0m         )\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2584\u001b[0m         )\n\u001b[0;32m   2585\u001b[0m     )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CropDiseaseModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([64, 7]) from checkpoint, the shape in current model is torch.Size([64, 6336]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([22, 6336]) from checkpoint, the shape in current model is torch.Size([22, 64])."
     ]
    }
   ],
   "source": [
    "model = CropDiseaseModel(num_classes=22)\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Package\\crop_disease_model.pth\", map_location='cpu'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_feature_extractor.0.weight', 'image_feature_extractor.0.bias', 'image_feature_extractor.3.weight', 'image_feature_extractor.3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias']\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(r\"C:\\Users\\Sujan.S\\Downloads\\DL Package1\\DL Package\\crop_disease_model.pth\", map_location='cpu')\n",
    "print([k for k in state_dict.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '→' (U+2192) (3942885209.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[37], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    x = CNN → flatten → fc1: Linear(6336, 64) → ReLU → fc2: Linear(64, 22)\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '→' (U+2192)\n"
     ]
    }
   ],
   "source": [
    "x = CNN → flatten → fc1: Linear(6336, 64) → ReLU → fc2: Linear(64, 22)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
